p8106\_hw1\_jsg2145
================
Jared Garfinkel
2/18/2020

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double()
    ## )

    ## See spec(...) for full column specifications.

## Part a

``` r
df_test = read_csv("./Data/solubility_test.csv") %>% 
  janitor::clean_names()
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double()
    ## )

    ## See spec(...) for full column specifications.

``` r
set.seed(22)
pred = predict(fit, newdata = df_test)

MSE = mean((df_test$solubility - pred)^2)

MSE
```

    ## [1] 0.5558898

The MSE for the linear model is 0.556.

## Part b

``` r
df = na.omit(df)

# Train data input matrix and response vector

x <- model.matrix(solubility~., df)[,-1]
y <- pull(df, solubility)


set.seed(22)
ridge.mod <- glmnet(x, y, 
                    standardize = TRUE, 
                    alpha = 0,
                    lambda = exp(seq(-10, 1, length = 100)))

mat.coef <- coef(ridge.mod)
dim(mat.coef)
```

    ## [1] 229 100

``` r
set.seed(22)
cv.ridge = cv.glmnet(x, y, 
                     type.measure = "mse", 
                     alpha = 0, 
                     lambda = exp(seq(-10, 1, length = 100)))

plot(cv.ridge)
```

<img src="p8106_hw1_jsg2145_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

``` r
plot(ridge.mod, xvar = "lambda", label = TRUE)
```

<img src="p8106_hw1_jsg2145_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

``` r
best.lambda <- cv.ridge$lambda.min
best.lambda
```

    ## [1] 0.06948345

The lambda from the ridge regression is 0.069.

``` r
set.seed(22)
predict(ridge.mod, s = best.lambda, type = "coefficients")
```

``` r
df_test = df_test %>% 
  na.omit()

set.seed(22)
x_test = model.matrix(solubility~., df_test)[,-1]
y_test = pull(df_test, solubility)

ridge_pred = predict(ridge.mod, s = best.lambda, newx = x_test)

mse_ridge = mean((y_test - ridge_pred)^2)
```

The MSE from the ridge model is 0.512.

## Part c

``` r
set.seed(22)
lasso.mod = glmnet(x, y, 
                    standardize = TRUE, 
                    alpha = 1,
                    lambda = exp(seq(-10, 1, length = 100)))
```

``` r
set.seed(22)
cv.lasso = cv.glmnet(x, y, 
                     type.measure = "mse", 
                     alpha = 1, 
                     lambda = exp(seq(-10, 1, length = 100)))

plot(cv.lasso)
```

<img src="p8106_hw1_jsg2145_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

``` r
lambda.lasso = cv.lasso$lambda.min
lambda.lasso
```

    ## [1] 0.005395326

The lambda returned by the lasso regression is 0.0054.

``` r
set.seed(22)
lasso.coef = predict(lasso.mod, s = lambda.lasso, type = "coefficients")
```

``` r
set.seed(22)
lasso_pred = predict(lasso.mod, s = lambda.lasso, newx = x_test)
lasso.mse = mean((lasso_pred - y_test)^2)
```

There are 141 non-zero coefficient estimates.

The MSE of the lasso model is 0.495.

## Part d, Principal Component Regression

``` r
set.seed(22)
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

pcr.fit = train(x, y,
                 method = "pcr",
                 tuneLength = nrow(df),
                 trControl = ctrl1,
                 preProc = c("center", "scale"))
```

``` r
ggplot(pcr.fit, highlight = TRUE)
```

<img src="p8106_hw1_jsg2145_files/figure-gfm/unnamed-chunk-15-1.png" width="90%" />

``` r
set.seed(22)
pcr.pred = predict(pcr.fit, newdata = df_test)
mse.pcr = mean((pcr.pred - y_test)^2)
```

The PSR model chooses 150 components with an MSE of 0.548.

## Part e, Results

While the MSE for a linear model is 0.556, this test error can be
reduced using further regression methods. Of the ridge, lasso, and
principal component regression methods, the lasso regression returns the
lowest MSE, suggesting a model with 141 components is the best model of
the three.

## Part f, Discussion

As mentioned above, the model with the lowest MSE is considered the best
predictive model, being the lasso model.
