---
title: "p8106_hw1_jsg2145"
author: "Jared Garfinkel"
date: "2/18/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(glmnet)
library(pls)
library(ISLR)
library(caret)
library(ModelMetrics)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

```{r, echo = FALSE, results = "hide"}
df = read_csv("./Data/solubility_train.csv") %>% 
  janitor::clean_names()
```

## Part a

```{r, echo = FALSE, results = "hide"}
lm.mod = lm(solubility ~ ., data = df)
summary(lm.mod)
```

```{r}
df_test = read_csv("./Data/solubility_test.csv") %>% 
  janitor::clean_names()

set.seed(22)
pred = predict(lm.mod, newdata = df_test)

MSE = mean((df_test$solubility - pred)^2)

MSE
```
The MSE for the linear model is `r round(MSE, digits = 3)`.

## Part b

```{r}
df = na.omit(df)

# Train data input matrix and response vector

x <- model.matrix(solubility~., df)[,-1]
y <- pull(df, solubility)


set.seed(22)
ridge.mod <- glmnet(x, y, 
                    standardize = TRUE, 
                    alpha = 0,
                    lambda = exp(seq(-10, 1, length = 100)))

mat.coef <- coef(ridge.mod)
dim(mat.coef)

set.seed(22)
cv.ridge = cv.glmnet(x, y, 
                     type.measure = "mse", 
                     alpha = 0, 
                     lambda = exp(seq(-10, 1, length = 100)))

plot(cv.ridge)
```

```{r, results = "hide"}
plot(ridge.mod, xvar = "lambda", label = TRUE)
```

```{r}
best.lambda <- cv.ridge$lambda.min
best.lambda
```

The lambda from the ridge regression is `r round(best.lambda, digits = 3)`.

```{r, results = "hide"}
set.seed(22)
predict(ridge.mod, s = best.lambda, type = "coefficients")
```

```{r}
df_test = df_test %>% 
  na.omit()

set.seed(22)
x_test = model.matrix(solubility~., df_test)[,-1]
y_test = pull(df_test, solubility)

ridge_pred = predict(ridge.mod, s = best.lambda, newx = x_test)

mse_ridge = mean((y_test - ridge_pred)^2)
```

The MSE from the ridge model is `r round(mse_ridge, digits = 3)`.

## Part c

```{r}
set.seed(22)
lasso.mod = glmnet(x, y, 
                    standardize = TRUE, 
                    alpha = 1,
                    lambda = exp(seq(-10, 1, length = 100)))
```

```{r}
set.seed(22)
cv.lasso = cv.glmnet(x, y, 
                     type.measure = "mse", 
                     alpha = 1, 
                     lambda = exp(seq(-10, 1, length = 100)))

plot(cv.lasso)
```

```{r}
lambda.lasso = cv.lasso$lambda.min
lambda.lasso
```

The lambda returned by the lasso regression is `r round(lambda.lasso, digits= 5)`.

```{r, results = "hide"}
set.seed(22)
lasso.coef = predict(lasso.mod, s = lambda.lasso, type = "coefficients")
```

```{r}
set.seed(22)
lasso_pred = predict(lasso.mod, s = lambda.lasso, newx = x_test)
lasso.mse = mean((lasso_pred - y_test)^2)
```

There are `r sum(abs(lasso.coef) > 0)` non-zero coefficient estimates.

The MSE of the lasso model is `r round(lasso.mse, digits = 3)`.

## Part d, Principal Component Regression

```{r, include = FALSE, eval = FALSE}
set.seed(22)
df = as_tibble(df)
pcr.mod <- pcr(solubility ~ .,
               data = df,
               scale = TRUE, 
               validation = "CV")

validationplot(pcr.mod, val.type="MSEP", legendpos = "topright")

y.pred.pcr <- predict(pcr.mod, newdata = df_test, 
                      ncomp = )

summary(pcr.mod)
```

```{r pcr with caret}
set.seed(22)
ctrl1 <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

pcr.mod = train(x, y,
                 method = "pcr",
                 tuneLength = nrow(df),
                 trControl = ctrl1,
                 preProc = c("center", "scale"))
```

```{r}
ggplot(pcr.mod, highlight = TRUE)
```

```{r}
set.seed(22)
pcr.pred = predict(pcr.mod, newdata = df_test)
mse.pcr = mean((pcr.pred - y_test)^2)
```

The PCR model chooses `r pcr.mod$bestTune` components with an MSE of `r round(mse.pcr, digits = 3)`.

## Now do it in caret

```{r}
set.seed(22)
ridge.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(-10, 1, length = 100))),
                   trControl = ctrl1)

predy2.ridge <- predict(ridge.fit$finalModel, newx = x_test, 
                        s = ridge.fit$bestTune$lambda, type = "response")

mse(y_test, predy2.ridge)

set.seed(22)
lasso.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(-10, 1, length = 100))),
                   trControl = ctrl1)

predy2.lasso <- predict(lasso.fit$finalModel, newx = x_test, 
                        s = lasso.fit$bestTune$lambda, type = "response")

mse(y_test, predy2.lasso)

set.seed(22)
lm.fit <- train(x, y,
                method = "lm",
                trControl = ctrl1)

predy2.lm <- predict(lm.fit$finalModel, newdata = data.frame(x_test))

mse(y_test, predy2.lm)
```


## Part e, Results

```{r}
resamp <- resamples(list(lasso = lasso.fit, 
                         ridge = ridge.fit, 
                         pcr = pcr.mod,
                         lm = lm.fit))

summary(resamp)

bwplot(resamp, metric = "RMSE")
```


While the MSE for a linear model is `r round(MSE, digits = 3)`, this test error can be reduced using further regression methods. Of the ridge, lasso, and principal component regression methods, the lasso regression returns the lowest MSE, suggesting a model with `r sum(abs(lasso.coef) > 0)` components is the best model of the three.

## Part f, Discussion

As mentioned above, the model with the lowest MSE is considered the best predictive model, being the lasso model.